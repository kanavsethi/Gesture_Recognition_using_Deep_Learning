{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random as rn\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "rn.seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open(\"./Data/train.csv\").readlines())\n",
    "val_doc = np.random.permutation(open(\"./Data/val.csv\").readlines())\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with some of the parts of the generator function such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size, image_size, img_idx, num_classes):\n",
    "    w, h = image_size\n",
    "\n",
    "    print(\"Source path = \", source_path, \"; batch size =\", batch_size)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t) / batch_size)\n",
    "        for batch in range(num_batches):\n",
    "            batch_data = np.zeros((batch_size, len(img_idx), w, h, 3))\n",
    "            batch_labels = np.zeros((batch_size, num_classes))\n",
    "            for folder in range(batch_size):\n",
    "                imgs = os.listdir(\n",
    "                    source_path + \"/\" + t[folder + (batch * batch_size)].split(\";\")[0]\n",
    "                )\n",
    "                for idx, item in enumerate(img_idx):\n",
    "                    image = Image.open(\n",
    "                        source_path\n",
    "                        + \"/\"\n",
    "                        + t[folder + (batch * batch_size)].strip().split(\";\")[0]\n",
    "                        + \"/\"\n",
    "                        + imgs[item]\n",
    "                    )\n",
    "\n",
    "                    if image.size[1] == 160:\n",
    "                        image = image.resize(image_size)\n",
    "                    else:\n",
    "                        image = image.resize(image_size)\n",
    "\n",
    "                    image = np.array(image).astype(np.float32)\n",
    "\n",
    "                    batch_data[folder, idx, :, :, 0] = image[:, :, 0] - 104\n",
    "                    batch_data[folder, idx, :, :, 1] = image[:, :, 1] - 117\n",
    "                    batch_data[folder, idx, :, :, 2] = image[:, :, 2] - 123\n",
    "\n",
    "                batch_labels[\n",
    "                    folder, int(t[folder + (batch * batch_size)].strip().split(\";\")[2])\n",
    "                ] = 1\n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "        if (len(t) % batch_size) != 0:\n",
    "            batch_data = np.zeros((len(t) % batch_size, len(img_idx), w, h, 3))\n",
    "            batch_labels = np.zeros((len(t) % batch_size, num_classes))\n",
    "            for folder in range(len(t) % batch_size):\n",
    "                imgs = os.listdir(\n",
    "                    source_path\n",
    "                    + \"/\"\n",
    "                    + t[folder + (num_batches * batch_size)].split(\";\")[0]\n",
    "                )\n",
    "                for idx, item in enumerate(img_idx):\n",
    "                    image = Image.open(\n",
    "                        source_path\n",
    "                        + \"/\"\n",
    "                        + t[folder + (num_batches * batch_size)].strip().split(\";\")[0]\n",
    "                        + \"/\"\n",
    "                        + imgs[item]\n",
    "                    )\n",
    "                    if image.size[1] == 160:\n",
    "                        image = image.resize(image_size)\n",
    "                    else:\n",
    "                        image = image.resize(image_size)\n",
    "\n",
    "                    image = np.array(image).astype(np.float32)\n",
    "\n",
    "                    batch_data[folder, idx, :, :, 0] = image[:, :, 0] - 104\n",
    "                    batch_data[folder, idx, :, :, 1] = image[:, :, 1] - 117\n",
    "                    batch_data[folder, idx, :, :, 2] = image[:, :, 2] - 123\n",
    "\n",
    "                batch_labels[\n",
    "                    folder,\n",
    "                    int(t[folder + (num_batches * batch_size)].strip().split(\";\")[2]),\n",
    "                ] = 1\n",
    "\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 70\n",
      "# image_size = (120, 120)\n",
      "# num_classes = 5\n",
      "# img_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28]\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = \"./Data/train\"\n",
    "val_path = \"./Data/val\"\n",
    "num_train_sequences = len(train_doc)\n",
    "print(\"# training sequences =\", num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print(\"# validation sequences =\", num_val_sequences)\n",
    "num_epochs = 70\n",
    "print(\"# epochs =\", num_epochs)\n",
    "image_size = (120, 120)\n",
    "print(\"# image_size =\", image_size)\n",
    "num_classes = 5\n",
    "print(\"# num_classes =\", num_classes)\n",
    "img_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28]\n",
    "print(\"# img_idx =\", img_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. You might want to use `TimeDistributed`, `GRU` and other RNN structures after doing transfer learning. Also remember that the last layer is the softmax. Remember that the network is designed in such a way that the model is able to fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizers\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg16\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VGG16\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Flatten, TimeDistributed\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "w, h = image_size\n",
    "\n",
    "base_model = VGG16(include_top=False, weights=\"imagenet\", input_shape=(w, h, 3))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "# x.add(Dropout(0.5))\n",
    "features = Dense(64, activation=\"relu\")(x)\n",
    "conv_model = Model(inputs=base_model.input, outputs=features)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(conv_model, input_shape=(15, 120, 120, 3)))\n",
    "model.add(GRU(32, return_sequences=True))\n",
    "model.add(GRU(16))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8, activation=\"relu\"))\n",
    "model.add(Dense(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 15, 64)           15009664  \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 15, 32)            9408      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 16)                2400      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 45        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,021,653\n",
      "Trainable params: 306,965\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "adam = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=adam,\n",
    "    loss=CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"categorical_accuracy\"],\n",
    ")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(\n",
    "    train_path, train_doc, batch_size, image_size, img_idx, num_classes\n",
    ")\n",
    "val_generator = generator(\n",
    "    val_path, val_doc, batch_size, image_size, img_idx, num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = (\n",
    "    \"./CheckPoints/\"\n",
    "    + \"model_init_conv_lstm\"\n",
    "    + \"_\"\n",
    "    + str(curr_dt_time).replace(\" \", \"\").replace(\":\", \"_\")\n",
    "    + \"/\"\n",
    ")\n",
    "\n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "\n",
    "filepath = (\n",
    "    model_name\n",
    "    + \"model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5\"\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\",\n",
    ")\n",
    "\n",
    "LR = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    mode=\"min\",\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0.00001,\n",
    ")\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences % batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences / batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences // batch_size) + 1\n",
    "\n",
    "if (num_val_sequences % batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences / batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences // batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Data/train ; batch size = 4\n",
      "Epoch 1/70\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nNo OpKernel was registered to support Op 'CudnnRNN' used by {{node CudnnRNN}} with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"gru\", seed2=0, is_training=true]\nRegistered devices: [CPU, GPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[CudnnRNN]]\n\t [[sequential/gru/PartitionedCall]] [Op:__inference_train_function_6857]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nNo OpKernel was registered to support Op 'CudnnRNN' used by {{node CudnnRNN}} with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"gru\", seed2=0, is_training=true]\nRegistered devices: [CPU, GPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[CudnnRNN]]\n\t [[sequential/gru/PartitionedCall]] [Op:__inference_train_function_6857]"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=num_epochs,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    class_weight=None,\n",
    "    workers=1,\n",
    "    initial_epoch=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
